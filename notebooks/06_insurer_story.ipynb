{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8edcd816",
   "metadata": {},
   "source": [
    "# Citi Bike: Usage patterns + insurer decision assets\n",
    "\n",
    "This notebook is built to work with the repo Make targets (e.g.make report-both YEARS=\"YYYY YYYY...\" MONTHS=\"1 2 .. 12\")\n",
    "It **always prefers the current run** (your `summaries/<RUN_TAG>/` folder) when you pass `YEARS/MONTHS`, so the report reflects exactly what you ingested.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1edc51",
   "metadata": {},
   "source": [
    "## How to run (recommended)\n",
    "\n",
    "From the repo root:\n",
    "```bash\n",
    "make all-both YEARS=\"YYYY YYYY...\" MONTHS=\"1 2 .. 12\"\n",
    "\n",
    "The Makefile sets environment variables (e.g. `CITIBIKE_PARQUET_DIR`, `CITIBIKE_YEARS`, `CITIBIKE_MONTHS`) which this notebook reads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup (STRICT + explicit risk variants): repo paths, run dirs, safe CSV loads ---\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional display in notebooks\n",
    "try:\n",
    "    from IPython.display import display, Markdown\n",
    "except Exception:\n",
    "    display = print\n",
    "    Markdown = lambda x: x\n",
    "\n",
    "# nbconvert-friendly\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
    "except Exception:\n",
    "    pass\n",
    "plt.ioff()\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"Makefile\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find repo root (Makefile) by walking up from:\\n\"\n",
    "        f\"  CWD={Path.cwd().resolve()}\"\n",
    "    )\n",
    "\n",
    "def _parse_int_list(val: str | None):\n",
    "    if val is None:\n",
    "        return None\n",
    "    s = str(val).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    parts = re.split(r\"[,\\s]+\", s)\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        if not p:\n",
    "            continue\n",
    "        try:\n",
    "            out.append(int(p))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out or None\n",
    "\n",
    "def read_csv_strict(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing required CSV: {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def read_csv_optional(path: Path) -> pd.DataFrame | None:\n",
    "    return pd.read_csv(path) if path.exists() else None\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "SUMMARIES_ROOT = REPO_ROOT / \"summaries\"\n",
    "\n",
    "PARQUET_DIR_ENV = (os.environ.get(\"CITIBIKE_PARQUET_DIR\") or \"\").strip()\n",
    "RUN_DIR_ENV     = (os.environ.get(\"CITIBIKE_RUN_DIR\") or \"\").strip()\n",
    "MODE_ENV        = (os.environ.get(\"CITIBIKE_MODE\") or os.environ.get(\"MODE\") or \"\").strip().lower()\n",
    "\n",
    "YEARS_FILTER  = _parse_int_list(os.environ.get(\"CITIBIKE_YEARS\")  or os.environ.get(\"YEARS\"))\n",
    "MONTHS_FILTER = _parse_int_list(os.environ.get(\"CITIBIKE_MONTHS\") or os.environ.get(\"MONTHS\"))\n",
    "\n",
    "PARQUET_DIR = Path(PARQUET_DIR_ENV) if PARQUET_DIR_ENV else Path()\n",
    "\n",
    "if RUN_DIR_ENV:\n",
    "    RUN_DIR = Path(RUN_DIR_ENV)\n",
    "else:\n",
    "    run_tag = PARQUET_DIR.name if str(PARQUET_DIR).strip() else \"\"\n",
    "    RUN_DIR = (SUMMARIES_ROOT / run_tag) if run_tag else Path()\n",
    "\n",
    "# Resolve relative paths against repo root\n",
    "if str(RUN_DIR).strip() and (not RUN_DIR.is_absolute()):\n",
    "    RUN_DIR = (REPO_ROOT / RUN_DIR).resolve()\n",
    "if str(PARQUET_DIR).strip() and (not PARQUET_DIR.is_absolute()):\n",
    "    PARQUET_DIR = (REPO_ROOT / PARQUET_DIR).resolve()\n",
    "\n",
    "# Strict folder checks\n",
    "if not SUMMARIES_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Expected summaries/ at: {SUMMARIES_ROOT} (run make summarize first)\")\n",
    "\n",
    "if not RUN_DIR.exists():\n",
    "    raise FileNotFoundError(f\"RUN_DIR not found: {RUN_DIR} (run make summarize first)\")\n",
    "\n",
    "if str(PARQUET_DIR).strip() and (not PARQUET_DIR.exists()):\n",
    "    raise FileNotFoundError(f\"PARQUET_DIR not found: {PARQUET_DIR} (run make ingest first)\")\n",
    "\n",
    "# Required run CSVs\n",
    "REQUIRED_RUN_FILES = [\n",
    "    \"citibike_trips_by_year.csv\",\n",
    "    \"citibike_trips_by_month.csv\",\n",
    "    \"citibike_trips_by_dow.csv\",\n",
    "    \"citibike_trips_by_hour.csv\",\n",
    "]\n",
    "missing = [f for f in REQUIRED_RUN_FILES if not (RUN_DIR / f).exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing required summary CSVs in:\\n  {RUN_DIR}\\nMissing: {missing}\\n\"\n",
    "        \"Run: make summarize-only (or make summarize / make all).\"\n",
    "    )\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"RUN_DIR:\", RUN_DIR)\n",
    "print(\"PARQUET_DIR:\", PARQUET_DIR if str(PARQUET_DIR).strip() else \"(not set)\")\n",
    "print(\"MODE (env):\", MODE_ENV or \"(not set)\")\n",
    "print(\"YEARS_FILTER:\", YEARS_FILTER, \"MONTHS_FILTER:\", MONTHS_FILTER)\n",
    "\n",
    "# Figures folder\n",
    "FIG_DIR = REPO_ROOT / \"reports\" / RUN_DIR.name / \"figures\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"FIG_DIR:\", FIG_DIR)\n",
    "\n",
    "def savefig(filename: str):\n",
    "    out = FIG_DIR / filename\n",
    "    plt.savefig(out, dpi=200, bbox_inches=\"tight\")\n",
    "    print(\"Saved:\", out)\n",
    "\n",
    "# Load per-run summaries\n",
    "df_year  = read_csv_strict(RUN_DIR / \"citibike_trips_by_year.csv\")\n",
    "df_month = read_csv_strict(RUN_DIR / \"citibike_trips_by_month.csv\")\n",
    "df_dow   = read_csv_strict(RUN_DIR / \"citibike_trips_by_dow.csv\")\n",
    "df_hour  = read_csv_strict(RUN_DIR / \"citibike_trips_by_hour.csv\")\n",
    "\n",
    "# Optional per-run outputs\n",
    "df_station = read_csv_optional(RUN_DIR / \"citibike_station_exposure.csv\")\n",
    "\n",
    "# Risk variants (overall + new per-year/per-month if present)\n",
    "df_risk_overall = read_csv_optional(RUN_DIR / \"station_risk_exposure_plus_crashproximity.csv\")\n",
    "df_risk_year    = read_csv_optional(RUN_DIR / \"station_risk_exposure_plus_crashproximity_by_year.csv\")\n",
    "df_risk_ym      = read_csv_optional(RUN_DIR / \"station_risk_exposure_plus_crashproximity_by_year_month.csv\")\n",
    "\n",
    "# Backward compat: keep df_risk pointing to \"best available\"\n",
    "df_risk = df_risk_year if df_risk_year is not None else df_risk_overall\n",
    "\n",
    "# Mode detection\n",
    "mode = (\n",
    "    str(df_year[\"mode\"].iloc[0]).lower()\n",
    "    if (\"mode\" in df_year.columns and len(df_year))\n",
    "    else (MODE_ENV or \"unknown\")\n",
    ")\n",
    "print(\"Detected mode:\", mode)\n",
    "\n",
    "# Apply optional filters defensively\n",
    "def _filter_year_month(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if YEARS_FILTER is not None and \"year\" in out.columns:\n",
    "        out[\"year\"] = pd.to_numeric(out[\"year\"], errors=\"coerce\")\n",
    "        out = out[out[\"year\"].isin(YEARS_FILTER)]\n",
    "    if MONTHS_FILTER is not None and \"month\" in out.columns:\n",
    "        out[\"month\"] = pd.to_numeric(out[\"month\"], errors=\"coerce\")\n",
    "        out = out[out[\"month\"].isin(MONTHS_FILTER)]\n",
    "    return out\n",
    "\n",
    "df_year  = _filter_year_month(df_year)\n",
    "df_month = _filter_year_month(df_month)\n",
    "df_dow   = _filter_year_month(df_dow)\n",
    "df_hour  = _filter_year_month(df_hour)\n",
    "\n",
    "# Helpful run label\n",
    "run_label = RUN_DIR.name\n",
    "\n",
    "print(\"\\nRisk inputs found:\")\n",
    "print(\" - df_risk_overall:\", \"YES\" if df_risk_overall is not None else \"NO\")\n",
    "print(\" - df_risk_year   :\", \"YES\" if df_risk_year is not None else \"NO\")\n",
    "print(\" - df_risk_ym     :\", \"YES\" if df_risk_ym is not None else \"NO\")\n",
    "print(\"Using df_risk = \", \"df_risk_year\" if df_risk is df_risk_year else (\"df_risk_overall\" if df_risk is df_risk_overall else \"None\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89aad0e",
   "metadata": {},
   "source": [
    "## Executive summary (optional)\n",
    "\n",
    "If you generated `summary_highlights.md` for this run, we show it here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566454aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quick text highlights from summarize script (if present) ---\n",
    "from pathlib import Path\n",
    "\n",
    "# Prefer the variable if it exists, else derive from RUN_DIR (which setup should define)\n",
    "hp = None\n",
    "if \"highlights_path\" in globals():\n",
    "    try:\n",
    "        hp = highlights_path\n",
    "    except Exception:\n",
    "        hp = None\n",
    "\n",
    "if hp is None:\n",
    "    if \"RUN_DIR\" in globals():\n",
    "        hp = Path(RUN_DIR) / \"summary_highlights.md\"\n",
    "    else:\n",
    "        # Last resort: try repo-relative \"summaries/latest\"\n",
    "        hp = Path(\"summaries/latest/summary_highlights.md\").resolve()\n",
    "\n",
    "if hp.exists():\n",
    "    txt = hp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    try:\n",
    "        display(Markdown(txt))\n",
    "    except Exception:\n",
    "        print(txt)\n",
    "else:\n",
    "    print(\"No summary_highlights.md found at:\", hp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd163137",
   "metadata": {},
   "source": [
    "## 1) Yearly usage (comparison across years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aa930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Trips by year (THIS RUN) ---\n",
    "g = df_year.copy()\n",
    "\n",
    "if \"year\" in g.columns:\n",
    "    # Clean and prepare data\n",
    "    g[\"year\"] = pd.to_numeric(g[\"year\"], errors=\"coerce\")\n",
    "    g = g.dropna(subset=[\"year\"]).sort_values(\"year\").reset_index(drop=True)\n",
    "    \n",
    "display(g)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "if \"year\" in g.columns and \"trips\" in g.columns:\n",
    "    # Extract values as plain lists (no pandas magic)\n",
    "    years = [int(y) for y in g[\"year\"]]\n",
    "    trips = [int(t) for t in g[\"trips\"]]\n",
    "    \n",
    "    # Plot with explicit year labels\n",
    "    plt.plot(years, trips, marker=\"o\", linewidth=2, markersize=8, color='#2E86AB')\n",
    "    plt.xticks(years, [str(y) for y in years])  # Explicit year labels\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add value labels on points\n",
    "    for year, trip in zip(years, trips):\n",
    "        plt.text(year, trip, f'{trip/1e6:.1f}M', \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.title(f\"Trips by year — mode={mode} ({run_label})\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Year\", fontsize=12)\n",
    "plt.ylabel(\"Trips\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "savefig(\"01_trips_by_year.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d53a71c",
   "metadata": {},
   "source": [
    "## 2) Month patterns (comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0391966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Trips by month (THIS RUN) ---\n",
    "m = df_month.copy()\n",
    "\n",
    "# Normalize month labels\n",
    "if \"month\" in m.columns:\n",
    "    m[\"month\"] = pd.to_numeric(m[\"month\"], errors=\"coerce\").astype(\"Int64\")\n",
    "if \"year\" in m.columns:\n",
    "    m[\"year\"] = pd.to_numeric(m[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "display(m.sort_values([c for c in [\"year\", \"month\"] if c in m.columns]))\n",
    "\n",
    "# If multiple years, show stacked-ish bars by year-month\n",
    "if {\"year\", \"month\", \"trips\"}.issubset(m.columns):\n",
    "    m2 = m.dropna(subset=[\"year\",\"month\"]).copy()\n",
    "    m2[\"ym\"] = m2[\"year\"].astype(int).astype(str) + \"-\" + m2[\"month\"].astype(int).astype(str).str.zfill(2)\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.bar(m2[\"ym\"], m2[\"trips\"])\n",
    "    plt.title(f\"Trips by year-month — mode={mode} ({run_label})\")\n",
    "    plt.xlabel(\"Year-Month\")\n",
    "    plt.ylabel(\"Trips\")\n",
    "    plt.xticks(rotation=75, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    savefig(\"02_trips_by_year_month.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Expected columns not present for trips-by-month plot. Have:\", list(m.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4dbcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Month-of-year seasonality (line chart per year) ---\n",
    "m = df_month.copy()\n",
    "if not {\"year\",\"month\",\"trips\"}.issubset(m.columns):\n",
    "    print(\"Month table missing required columns:\", list(m.columns))\n",
    "else:\n",
    "    m[\"year\"]  = pd.to_numeric(m[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    m[\"month\"] = pd.to_numeric(m[\"month\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    m = m.dropna(subset=[\"year\",\"month\"]).copy()\n",
    "    m = m.sort_values([\"year\",\"month\"])\n",
    "\n",
    "    pivot = m.pivot_table(index=\"month\", columns=\"year\", values=\"trips\", aggfunc=\"sum\").sort_index()\n",
    "    display(pivot)\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    for y in pivot.columns:\n",
    "        plt.plot(pivot.index.astype(int), pivot[y], marker=\"o\", label=str(int(y)))\n",
    "    plt.title(f\"Trips by month-of-year — mode={mode} ({run_label})\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Trips\")\n",
    "    plt.xticks(range(1,13))\n",
    "    plt.legend(title=\"Year\", ncol=2, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    savefig(\"03_month_by_year_lines.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3147d38",
   "metadata": {},
   "source": [
    "## 3) Day-of-week patterns (comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73463fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Day-of-week patterns (comparison across years; robust even if dow_name missing) ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if df_dow is None or len(df_dow) == 0:\n",
    "    print(\"df_dow is None/empty; skipping DOW comparison.\")\n",
    "else:\n",
    "    d = df_dow.copy()\n",
    "\n",
    "    # ---- Required columns ----\n",
    "    req = {\"dow\", \"trips\"}\n",
    "    missing = [c for c in req if c not in d.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"df_dow missing required columns {missing}. Have: {list(d.columns)}\")\n",
    "\n",
    "    # Ensure year exists for \"comparison\"\n",
    "    if \"year\" not in d.columns:\n",
    "        print(\"df_dow has no 'year' column; cannot do year comparison.\")\n",
    "        display(d.head(20))\n",
    "    else:\n",
    "        # ---- Clean types ----\n",
    "        d[\"dow\"] = pd.to_numeric(d[\"dow\"], errors=\"coerce\")\n",
    "        d[\"trips\"] = pd.to_numeric(d[\"trips\"], errors=\"coerce\").fillna(0)\n",
    "        d[\"year\"] = pd.to_numeric(d[\"year\"], errors=\"coerce\")\n",
    "        d = d.dropna(subset=[\"dow\", \"year\"]).copy()\n",
    "        d[\"dow\"] = d[\"dow\"].astype(int)\n",
    "        d[\"year\"] = d[\"year\"].astype(int)\n",
    "\n",
    "        # ---- Add dow_name if missing (deterministic mapping) ----\n",
    "        if \"dow_name\" not in d.columns:\n",
    "            dow_map = {\n",
    "                0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\", 3: \"Thursday\",\n",
    "                4: \"Friday\", 5: \"Saturday\", 6: \"Sunday\",\n",
    "            }\n",
    "            d[\"dow_name\"] = d[\"dow\"].map(dow_map).fillna(d[\"dow\"].astype(str))\n",
    "\n",
    "        # ---- Add week_part if missing ----\n",
    "        if \"week_part\" not in d.columns:\n",
    "            d[\"week_part\"] = np.where(d[\"dow\"] >= 5, \"weekend\", \"weekday\")\n",
    "\n",
    "        years = sorted(d[\"year\"].unique().tolist())\n",
    "        print(\"Years present in df_dow:\", years)\n",
    "\n",
    "        # ---- TABLE 1: pivot (dow_name x year) ----\n",
    "        pivot = (\n",
    "            d.pivot_table(index=[\"dow\", \"dow_name\"], columns=\"year\", values=\"trips\", aggfunc=\"sum\")\n",
    "             .sort_index(level=0)\n",
    "        )\n",
    "        print(\"\\nTrips by day-of-week (pivot):\")\n",
    "        display(pivot)\n",
    "\n",
    "        # ---- TABLE 2: shares within each year (percent of yearly total) ----\n",
    "        shares = (\n",
    "            d.groupby([\"year\", \"dow\", \"dow_name\"], as_index=False)[\"trips\"]\n",
    "             .sum()\n",
    "             .sort_values([\"year\", \"dow\"])\n",
    "        )\n",
    "        shares[\"pct_of_year\"] = shares[\"trips\"] / shares.groupby(\"year\")[\"trips\"].transform(\"sum\") * 100.0\n",
    "        share_pivot = (\n",
    "            shares.pivot_table(index=[\"dow\", \"dow_name\"], columns=\"year\", values=\"pct_of_year\", aggfunc=\"first\")\n",
    "                  .sort_index(level=0)\n",
    "        )\n",
    "        print(\"\\nShare of trips within each year (%):\")\n",
    "        display(share_pivot.round(2))\n",
    "\n",
    "        # ---- PLOT 1: absolute trips per DOW for each year ----\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        x_labels = [f\"{n}\" for n in pivot.index.get_level_values(\"dow_name\")]\n",
    "        x = np.arange(len(x_labels))\n",
    "\n",
    "        for yy in years:\n",
    "            if yy in pivot.columns:\n",
    "                plt.plot(x, pivot[yy].values, marker=\"o\", label=str(yy))\n",
    "\n",
    "        plt.xticks(x, x_labels, rotation=30, ha=\"right\")\n",
    "        plt.xlabel(\"Day of Week\")\n",
    "        plt.ylabel(\"Trips\")\n",
    "        plt.title(f\"Trips by Day of Week — year comparison — mode={mode} ({run_label})\")\n",
    "        plt.grid(alpha=0.25, axis=\"y\")\n",
    "        plt.legend(title=\"Year\")\n",
    "        plt.tight_layout()\n",
    "        savefig(\"10_dow_year_comparison_abs.png\")\n",
    "        plt.show()\n",
    "\n",
    "        # ---- PLOT 2: % share per DOW for each year (better for pattern comparison) ----\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        for yy in years:\n",
    "            if yy in share_pivot.columns:\n",
    "                plt.plot(x, share_pivot[yy].values, marker=\"o\", label=str(yy))\n",
    "\n",
    "        plt.xticks(x, x_labels, rotation=30, ha=\"right\")\n",
    "        plt.xlabel(\"Day of Week\")\n",
    "        plt.ylabel(\"Share of trips within year (%)\")\n",
    "        plt.title(f\"Day-of-week pattern (% within year) — year comparison — mode={mode} ({run_label})\")\n",
    "        plt.grid(alpha=0.25, axis=\"y\")\n",
    "        plt.legend(title=\"Year\")\n",
    "        plt.tight_layout()\n",
    "        savefig(\"10_dow_year_comparison_pct.png\")\n",
    "        plt.show()\n",
    "\n",
    "        # ---- OPTIONAL: weekday vs weekend bar per year ----\n",
    "        wp = (\n",
    "            d.groupby([\"year\", \"week_part\"], as_index=False)[\"trips\"]\n",
    "             .sum()\n",
    "             .sort_values([\"year\", \"week_part\"])\n",
    "        )\n",
    "        wp[\"pct\"] = wp[\"trips\"] / wp.groupby(\"year\")[\"trips\"].transform(\"sum\") * 100.0\n",
    "\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        # make consistent order\n",
    "        order = [\"weekday\", \"weekend\"]\n",
    "        for yy in years:\n",
    "            sub = wp[wp[\"year\"] == yy].copy()\n",
    "            sub[\"week_part\"] = pd.Categorical(sub[\"week_part\"], categories=order, ordered=True)\n",
    "            sub = sub.sort_values(\"week_part\")\n",
    "            plt.plot(sub[\"week_part\"].astype(str), sub[\"pct\"].values, marker=\"o\", label=str(yy))\n",
    "\n",
    "        plt.xlabel(\"Week Part\")\n",
    "        plt.ylabel(\"Share of trips within year (%)\")\n",
    "        plt.title(f\"Weekday vs Weekend share — year comparison — mode={mode} ({run_label})\")\n",
    "        plt.grid(alpha=0.25, axis=\"y\")\n",
    "        plt.legend(title=\"Year\")\n",
    "        plt.tight_layout()\n",
    "        savefig(\"10_weekday_weekend_year_comparison_pct.png\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e151ebc",
   "metadata": {},
   "source": [
    "## 4) Hour-of-day patterns (comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Trips by hour: compare years (table + plot) ---\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "h = df_hour.copy()\n",
    "\n",
    "hour_col = \"hour\" if \"hour\" in h.columns else None\n",
    "trips_col = \"trips\" if \"trips\" in h.columns else None\n",
    "year_col  = \"year\" if \"year\" in h.columns else None\n",
    "\n",
    "part_col = None\n",
    "for c in [\"week_part\", \"segment\", \"is_weekend\"]:\n",
    "    if c in h.columns:\n",
    "        part_col = c\n",
    "        break\n",
    "\n",
    "if not (hour_col and trips_col and year_col):\n",
    "    print(\"Hour table missing required columns. Have:\", list(h.columns))\n",
    "else:\n",
    "    # Clean\n",
    "    h[hour_col] = pd.to_numeric(h[hour_col], errors=\"coerce\")\n",
    "    h[trips_col] = pd.to_numeric(h[trips_col], errors=\"coerce\")\n",
    "    h[year_col] = pd.to_numeric(h[year_col], errors=\"coerce\")\n",
    "    h = h.dropna(subset=[hour_col, trips_col, year_col]).copy()\n",
    "    h[hour_col] = h[hour_col].astype(int)\n",
    "    h[year_col] = h[year_col].astype(int)\n",
    "\n",
    "    years = sorted(h[year_col].unique().tolist())\n",
    "    print(\"Years present in df_hour:\", years)\n",
    "\n",
    "    # ---- TABLES ----\n",
    "    print(\"\\nSample rows per year (first 8 hours for each year):\")\n",
    "    display(\n",
    "        h.sort_values([year_col, hour_col])\n",
    "         .groupby(year_col, as_index=False)\n",
    "         .head(8)\n",
    "    )\n",
    "\n",
    "# #    if part_col:\n",
    "# #        print(f\"\\nPivot table: trips by hour × year × {part_col}\")\n",
    "#         pivot = (\n",
    "#             h.pivot_table(index=[hour_col, part_col], columns=year_col, values=trips_col, aggfunc=\"sum\")\n",
    "#              .sort_index()\n",
    "#         )\n",
    "#         display(pivot)\n",
    "#     else:\n",
    "#         print(\"\\nPivot table: trips by hour × year\")\n",
    "#         pivot = (\n",
    "#             h.pivot_table(index=hour_col, columns=year_col, values=trips_col, aggfunc=\"sum\")\n",
    "#              .sort_index()\n",
    "#         )\n",
    "#         display(pivot)\n",
    "\n",
    "    # ---- PLOT ----\n",
    "    plt.figure(figsize=(10,4))\n",
    "    if part_col:\n",
    "        for (yy, seg), sub in h.groupby([year_col, part_col]):\n",
    "            sub = sub.sort_values(hour_col)\n",
    "            plt.plot(sub[hour_col], sub[trips_col], marker=\"o\", label=f\"{yy} | {seg}\")\n",
    "    else:\n",
    "        for yy, sub in h.groupby(year_col):\n",
    "            sub = sub.sort_values(hour_col)\n",
    "            plt.plot(sub[hour_col], sub[trips_col], marker=\"o\", label=str(yy))\n",
    "\n",
    "    plt.title(f\"Trips by hour (year comparison) — mode={mode} ({run_label})\")\n",
    "    plt.xlabel(\"Hour\")\n",
    "    plt.ylabel(\"Trips\")\n",
    "    plt.xticks(range(0,24,1))\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    savefig(\"05_trips_by_hour_year_comparison.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf77810",
   "metadata": {},
   "source": [
    "## 6) Crash proximity / risk proxy (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac3431",
   "metadata": {},
   "source": [
    "### 6.1 Station-level prioritization (make the proxy usable)\n",
    "\n",
    "The raw file includes **crash counts within 250m/500m** of each station plus **trip volume**.  \n",
    "To avoid misleading outliers (e.g., stations with only a handful of trips), we:\n",
    "\n",
    "- keep IDs as **strings** (so `2231.10` doesn’t turn into `2231.1`)\n",
    "- recompute crash-rates per 100k trips (defensive)\n",
    "- rank **exposure** (most trips) separately from **risk rate** (filtered to stations with enough trips)\n",
    "\n",
    "You can tune `MIN_TRIPS_FOR_RATE` depending on how conservative you want to be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "risk_metrics_header",
   "metadata": {},
   "source": [
    "### 6.2 Alternative Risk Metrics (Intuitive Formats)\n",
    "\n",
    "The standard `per_100k_trips` metric is useful for technical analysis, but can be hard to interpret.\n",
    "This section adds alternative formats that are more intuitive for different audiences:\n",
    "\n",
    "- **Percentage** (crash_rate_pct): Good for executives\n",
    "- **Trips per crash**: Most intuitive for general public\n",
    "- **Risk level**: Classification (Very Low → Very High)\n",
    "- **Risk score**: 0-100 ranking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7002b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---  Crash proximity risk proxy (NYC only): YEARLY comparison ---\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MIN_TRIPS_FOR_CREDIBLE_DISPLAY = 5000\n",
    "TOP_N = 20\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "        \n",
    "if 'RUN_DIR' in globals():\n",
    "            # Find any scorecard file\n",
    "            scorecard_files = list(Path(RUN_DIR).glob(\"axa_partner_scorecard_*m.csv\"))\n",
    "            if scorecard_files:\n",
    "                df_score = pd.read_csv(scorecard_files[0])\n",
    "            else:\n",
    "                print(\"   No scorecard files found in RUN_DIR\")\n",
    "                df_score = None     \n",
    "\n",
    "# Only proceed if we have data\n",
    "if df_score is not None and not df_score.empty:\n",
    "    print(f\"Using scorecard with {len(df_score):,} rows\")\n",
    "    \n",
    "    # Normalize column names if needed\n",
    "    if \"station_id\" not in df_score.columns and \"start_station_id\" in df_score.columns:\n",
    "        df_score = df_score.rename(columns={\"start_station_id\": \"station_id\"})\n",
    "    if \"station_name\" not in df_score.columns and \"start_station_name\" in df_score.columns:\n",
    "        df_score = df_score.rename(columns={\"start_station_name\": \"station_name\"})\n",
    "    \n",
    "    # Check if we have year-level data\n",
    "    if \"year\" not in df_score.columns:\n",
    "        print(\"Scorecard has no 'year' column (overall aggregate only).\")\n",
    "    else:\n",
    "        # Use PRE-COMPUTED EB values from scorecard\n",
    "        required_cols = [\"eb_risk_rate_per_100k_trips\", \"expected_incidents_proxy\"]\n",
    "        missing = [c for c in required_cols if c not in df_score.columns]\n",
    "        \n",
    "        if missing:\n",
    "            print(f\"WARNING: Scorecard missing EB columns: {missing}\")\n",
    "            print(\"Did build_axa_scorecard.py run successfully?\")\n",
    "        else:\n",
    "            r = df_score.copy()\n",
    "            \n",
    "            # Filter to credible stations for display\n",
    "            r[\"trips\"] = pd.to_numeric(r.get(\"exposure_trips\", r.get(\"trips\", 0)), errors=\"coerce\")\n",
    "            r = r[r[\"trips\"] >= MIN_TRIPS_FOR_CREDIBLE_DISPLAY].copy()\n",
    "            \n",
    "            years = sorted(r[\"year\"].dropna().unique().tolist())\n",
    "            \n",
    "            print(f\"\\n=== Yearly Risk Analysis (using scorecard EB estimates) ===\")\n",
    "            print(f\"Stations with ≥{MIN_TRIPS_FOR_CREDIBLE_DISPLAY:,} trips\")\n",
    "            \n",
    "            yearly_burden = []\n",
    "            \n",
    "            for yy in years:\n",
    "                ry = r[r[\"year\"] == yy].copy()\n",
    "                \n",
    "                if ry.empty:\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\n--- Year {int(yy)} ---\")\n",
    "                print(f\"Top {TOP_N} by exposure:\")\n",
    "                display(ry.sort_values(\"trips\", ascending=False).head(TOP_N)[\n",
    "                    [\"mode\", \"year\", \"station_id\", \"station_name\", \"trips\"]\n",
    "                ])\n",
    "                \n",
    "                print(f\"Top {TOP_N} by EB risk rate:\")\n",
    "                display(ry.sort_values(\"eb_risk_rate_per_100k_trips\", ascending=False).head(TOP_N)[\n",
    "                    [\"mode\", \"year\", \"station_id\", \"station_name\", \"trips\", \n",
    "                     \"crash_count\", \"eb_risk_rate_per_100k_trips\"]\n",
    "                ])\n",
    "                \n",
    "                # Yearly burden (sum of expected incidents)\n",
    "                total_burden = ry[\"expected_incidents_proxy\"].sum()\n",
    "                yearly_burden.append({\"year\": int(yy), \"eb_expected_crashes\": total_burden})\n",
    "            \n",
    "            # Plot yearly burden\n",
    "            if yearly_burden:\n",
    "                yb = pd.DataFrame(yearly_burden).sort_values(\"year\")\n",
    "                \n",
    "                plt.figure(figsize=(9, 4))\n",
    "                plt.bar(yb[\"year\"].astype(str), yb[\"eb_expected_crashes\"].values)\n",
    "                plt.title(f\"Yearly EB crash-proxy burden — mode={mode}\")\n",
    "                plt.xlabel(\"Year\")\n",
    "                plt.ylabel(\"Expected incidents (sum across stations)\")\n",
    "                plt.tight_layout()\n",
    "                savefig(\"09_yearly_EB_crash_proxy_burden.png\")\n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"\\nTotal EB expected crashes by year:\")\n",
    "                display(yb)\n",
    "else:\n",
    "    print(\" Skipping yearly risk analysis - no scorecard data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b14c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exposure vs Risk \"Zones\" (EB-smoothed quadrant view, radius-aware) ---\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MIN_TRIPS_FOR_ZONES = 5000\n",
    "SCALE = 100000.0\n",
    "\n",
    "_RADIUS_RE = re.compile(r\"^\\s*(\\d+(?:\\.\\d+)?)\\s*(m|km)?\\s*$\", re.IGNORECASE)\n",
    "\n",
    "def parse_radius_to_m(raw: str) -> int:\n",
    "    s = (raw or \"\").strip().lower()\n",
    "    if s in (\"\", \"auto\", \"max\"):\n",
    "        return -1\n",
    "    m = _RADIUS_RE.match(s)\n",
    "    if not m:\n",
    "        return -1\n",
    "    val = float(m.group(1))\n",
    "    unit = (m.group(2) or \"m\").lower()\n",
    "    meters = val * (1000.0 if unit == \"km\" else 1.0)\n",
    "    return int(round(meters)) if meters > 0 else -1\n",
    "\n",
    "def extract_radius_from_scorecard(df: pd.DataFrame) -> int | None:\n",
    "    \"\"\"Extract radius from crash_count column or scoring_strategy\"\"\"\n",
    "    # Try to find crashes_within_{R}m column in raw risk data\n",
    "    for c in df.columns:\n",
    "        mm = re.match(r\"^crashes_within_(\\d+)m$\", str(c))\n",
    "        if mm:\n",
    "            return int(mm.group(1))\n",
    "    \n",
    "    # Try to extract from scoring_strategy if present\n",
    "    if \"scoring_strategy\" in df.columns and len(df) > 0:\n",
    "        # scoring_strategy might contain info about radius used\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Determine which data source to use\n",
    "use_scorecard = False\n",
    "radius_used = None\n",
    "\n",
    "if df_score is not None and len(df_score) > 0:\n",
    "    print(\" Using SCORECARD data (EB-smoothed)\")\n",
    "    r = df_score.copy()\n",
    "    use_scorecard = True\n",
    "    \n",
    "    # Normalize column names\n",
    "    if \"station_id\" not in r.columns and \"start_station_id\" in r.columns:\n",
    "        r = r.rename(columns={\"start_station_id\": \"station_id\"})\n",
    "    if \"station_name\" not in r.columns and \"start_station_name\" in r.columns:\n",
    "        r = r.rename(columns={\"start_station_name\": \"station_name\"})\n",
    "    \n",
    "    # Get exposure (scorecard uses exposure_trips)\n",
    "    if \"exposure_trips\" in r.columns:\n",
    "        r[\"trips\"] = pd.to_numeric(r[\"exposure_trips\"], errors=\"coerce\")\n",
    "    elif \"trips\" not in r.columns:\n",
    "        print(\"ERROR: No trips/exposure_trips column in scorecard\")\n",
    "        r = None\n",
    "    \n",
    "    # Use EB-smoothed risk rate if available\n",
    "    if r is not None and \"eb_risk_rate_per_100k_trips\" in r.columns:\n",
    "        r[\"risk_rate\"] = pd.to_numeric(r[\"eb_risk_rate_per_100k_trips\"], errors=\"coerce\")\n",
    "        risk_metric_name = \"EB-smoothed risk rate per 100k trips\"\n",
    "    elif r is not None and \"risk_rate_per_100k_trips\" in r.columns:\n",
    "        r[\"risk_rate\"] = pd.to_numeric(r[\"risk_rate_per_100k_trips\"], errors=\"coerce\")\n",
    "        risk_metric_name = \"Raw risk rate per 100k trips\"\n",
    "        print(\"  Using raw risk rate (EB column not found)\")\n",
    "    else:\n",
    "        print(\"ERROR: No risk rate column found in scorecard\")\n",
    "        r = None\n",
    "    \n",
    "    # Try to determine radius from scorecard\n",
    "    if r is not None and \"crash_count\" in r.columns:\n",
    "        # The scorecard was built from a specific radius\n",
    "        # Try to extract it from the filename or strategy\n",
    "        env_raw = os.environ.get(\"AXA_RADIUS\", \"auto\")\n",
    "        radius_used = parse_radius_to_m(env_raw)\n",
    "        if radius_used == -1:\n",
    "            # Parse from scorecard filename if loaded via glob\n",
    "            radius_used = 500  # default assumption\n",
    "        risk_metric_name = f\"{risk_metric_name} ({radius_used}m radius)\"\n",
    "\n",
    "elif df_risk is not None:\n",
    "    print(\"  FALLBACK: Using RAW risk data (no scorecard available)\")\n",
    "    r = df_risk.copy()\n",
    "    \n",
    "    # Normalize column names\n",
    "    if \"station_id\" not in r.columns and \"start_station_id\" in r.columns:\n",
    "        r = r.rename(columns={\"start_station_id\": \"station_id\"})\n",
    "    if \"station_name\" not in r.columns and \"start_station_name\" in r.columns:\n",
    "        r = r.rename(columns={\"start_station_name\": \"station_name\"})\n",
    "    \n",
    "    r[\"trips\"] = pd.to_numeric(r.get(\"trips\", pd.NA), errors=\"coerce\")\n",
    "    \n",
    "    # Find available radii\n",
    "    avail_radii = []\n",
    "    for c in r.columns:\n",
    "        mm = re.match(r\"^crashes_within_(\\d+)m$\", str(c))\n",
    "        if mm:\n",
    "            avail_radii.append(int(mm.group(1)))\n",
    "    \n",
    "    if not avail_radii:\n",
    "        print(\"ERROR: No crashes_within_<R>m columns found\")\n",
    "        r = None\n",
    "    else:\n",
    "        # Choose radius\n",
    "        env_raw = os.environ.get(\"AXA_RADIUS\", \"auto\")\n",
    "        wanted = parse_radius_to_m(env_raw)\n",
    "        radius_used = max(avail_radii) if wanted == -1 else (wanted if wanted in avail_radii else (500 if 500 in avail_radii else max(avail_radii)))\n",
    "        \n",
    "        crash_col = f\"crashes_within_{radius_used}m\"\n",
    "        \n",
    "        # Ensure crash column is numeric\n",
    "        if crash_col not in r.columns:\n",
    "            r[crash_col] = 0\n",
    "        r[crash_col] = pd.to_numeric(r[crash_col], errors=\"coerce\").fillna(0).astype(int)\n",
    "        \n",
    "        # Compute RAW rate\n",
    "        r[\"risk_rate\"] = (r[crash_col] / r[\"trips\"].replace({0: np.nan})) * SCALE\n",
    "        r[\"risk_rate\"] = pd.to_numeric(r[\"risk_rate\"], errors=\"coerce\").fillna(0.0)\n",
    "        risk_metric_name = f\"Raw crash rate per 100k trips ({radius_used}m radius)\"\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: No data available (neither scorecard nor risk file)\")\n",
    "    r = None\n",
    "\n",
    "# Proceed with plotting if we have data\n",
    "if r is not None:\n",
    "    # Clean data\n",
    "    r = r.dropna(subset=[\"trips\", \"risk_rate\"]).copy()\n",
    "    r = r[r[\"trips\"] > 0].copy()\n",
    "    r = r[r[\"trips\"] >= MIN_TRIPS_FOR_ZONES].copy()\n",
    "    \n",
    "    if len(r) == 0:\n",
    "        print(f\"No stations meet trips ≥ {MIN_TRIPS_FOR_ZONES} — skipping zones plot.\")\n",
    "    else:\n",
    "        print(f\"Plotting {len(r):,} stations with ≥ {MIN_TRIPS_FOR_ZONES:,} trips\")\n",
    "        \n",
    "        x = r[\"trips\"].astype(float)\n",
    "        y = r[\"risk_rate\"].astype(float)\n",
    "        \n",
    "        x_med = float(np.nanmedian(x))\n",
    "        y_med = float(np.nanmedian(y))\n",
    "        \n",
    "        print(f\"Median exposure: {x_med:,.0f} trips\")\n",
    "        print(f\"Median risk: {y_med:.2f}\")\n",
    "        \n",
    "        # Classify into zones\n",
    "        def zone(row):\n",
    "            hi_x = float(row[\"trips\"]) >= x_med\n",
    "            hi_y = float(row[\"risk_rate\"]) >= y_med\n",
    "            if hi_x and hi_y:\n",
    "                return \"High exposure / High risk\"\n",
    "            if hi_x and (not hi_y):\n",
    "                return \"High exposure / Lower risk\"\n",
    "            if (not hi_x) and hi_y:\n",
    "                return \"Lower exposure / High risk\"\n",
    "            return \"Lower exposure / Lower risk\"\n",
    "        \n",
    "        r[\"zone\"] = r.apply(zone, axis=1)\n",
    "        \n",
    "        # Show zone distribution\n",
    "        print(\"\\nZone distribution:\")\n",
    "        display(r[\"zone\"].value_counts().to_frame(\"stations\"))\n",
    "        \n",
    "        # Plot quadrants\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        \n",
    "        # Define colors for each zone\n",
    "        zone_colors = {\n",
    "            \"High exposure / High risk\": \"#d62728\",      # red\n",
    "            \"High exposure / Lower risk\": \"#2ca02c\",     # green\n",
    "            \"Lower exposure / High risk\": \"#ff7f0e\",     # orange\n",
    "            \"Lower exposure / Lower risk\": \"#1f77b4\"     # blue\n",
    "        }\n",
    "        \n",
    "        for z, sub in r.groupby(\"zone\"):\n",
    "            plt.scatter(\n",
    "                sub[\"trips\"], \n",
    "                sub[\"risk_rate\"], \n",
    "                alpha=0.6, \n",
    "                s=60,\n",
    "                label=f\"{z} (n={len(sub)})\",\n",
    "                color=zone_colors.get(z, \"#999999\")\n",
    "            )\n",
    "        \n",
    "        # Add median lines\n",
    "        plt.axvline(x_med, linestyle=\"--\", color=\"gray\", alpha=0.7, linewidth=1.5, label=f\"Median exposure ({x_med:,.0f})\")\n",
    "        plt.axhline(y_med, linestyle=\"--\", color=\"gray\", alpha=0.7, linewidth=1.5, label=f\"Median risk ({y_med:.2f})\")\n",
    "        \n",
    "        # Formatting\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Exposure (trips, log scale)\", fontsize=11)\n",
    "        plt.ylabel(risk_metric_name, fontsize=11)\n",
    "        \n",
    "        source_label = \"EB-smoothed\" if use_scorecard else \"Raw data\"\n",
    "        plt.title(\n",
    "            f\"Exposure vs Risk Zones — {source_label}\\n\"\n",
    "            f\"(stations with ≥ {MIN_TRIPS_FOR_ZONES:,} trips, mode={mode})\",\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\"\n",
    "        )\n",
    "        \n",
    "        plt.legend(fontsize=8, loc=\"best\")\n",
    "        plt.grid(True, alpha=0.3, linestyle=\":\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save\n",
    "        filename = f\"10_zones_exposure_vs_risk_{radius_used}m_trips_ge_{MIN_TRIPS_FOR_ZONES}_{'EB' if use_scorecard else 'raw'}.png\"\n",
    "        savefig(filename)\n",
    "        plt.show()\n",
    "        \n",
    "        # Table of \"high-high\" quadrant (prevention priority)\n",
    "        hh = r[(x >= x_med) & (y >= y_med)].copy()\n",
    "        hh = hh.sort_values([\"risk_rate\", \"trips\"], ascending=False).head(15)\n",
    "        \n",
    "        print(f\"\\n HIGH PRIORITY STATIONS (High exposure + High risk, top 15):\")\n",
    "        display_cols = [\"mode\", \"station_id\", \"station_name\", \"trips\", \"risk_rate\"]\n",
    "        if \"crash_count\" in hh.columns:\n",
    "            display_cols.append(\"crash_count\")\n",
    "        if \"credibility_flag\" in hh.columns:\n",
    "            display_cols.append(\"credibility_flag\")\n",
    "        \n",
    "        display(hh[[c for c in display_cols if c in hh.columns]])\n",
    "        \n",
    "        # Additional insights\n",
    "        if use_scorecard:\n",
    "            print(\"\\n EB Smoothing Info:\")\n",
    "            if \"eb_m_prior_used\" in r.columns:\n",
    "                m_val = r[\"eb_m_prior_used\"].iloc[0]\n",
    "                print(f\"  Prior strength (m): {m_val:,.0f}\")\n",
    "            if \"scoring_strategy\" in r.columns:\n",
    "                strat = r[\"scoring_strategy\"].iloc[0]\n",
    "                print(f\"  Strategy: {strat}\")\n",
    "        \n",
    "        print(f\"\\n Business Interpretation:\")\n",
    "        print(f\"  • {len(hh)} high-priority stations → Prevention pilots & safety interventions\")\n",
    "        high_exp_low_risk = len(r[(x >= x_med) & (y < y_med)])\n",
    "        print(f\"  • {high_exp_low_risk} high-exposure/low-risk stations → Product upsell targets\")\n",
    "\n",
    "else:\n",
    "    print(\"  Skipping zones plot due to data issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ad14e",
   "metadata": {},
   "source": [
    "## 7) AXA Partner Decision Assets: Where + When + What\n",
    "\n",
    "This section turns your outputs into **two decision assets**:\n",
    "\n",
    "- **WHERE to focus** (stations): `axa_partner_scorecard_500m.csv`\n",
    "- **WHEN to activate** (time windows): `axa_target_windows.csv`\n",
    "\n",
    "Both files are produced by the Make targets you already ran.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462c6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AXA Decision Assets (INSURER-READY): WHERE + WHEN + WHAT (clean tables, no booleans/percentile columns) ---\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "MIN_TRIPS = 5000\n",
    "TOP_N = 20\n",
    "\n",
    "# -----------------------\n",
    "# Helpers: load if missing\n",
    "# -----------------------\n",
    "_SCORE_RE = re.compile(r\"^axa_partner_scorecard_(\\d+)m\\.csv$\")\n",
    "\n",
    "def _available_scorecards(run_dir: Path) -> list[int]:\n",
    "    radii = []\n",
    "    for p in run_dir.glob(\"axa_partner_scorecard_*m.csv\"):\n",
    "        m = _SCORE_RE.match(p.name)\n",
    "        if m:\n",
    "            radii.append(int(m.group(1)))\n",
    "    return sorted(set(radii))\n",
    "\n",
    "def _pick_scorecard_path(run_dir: Path) -> Path | None:\n",
    "    radii = _available_scorecards(run_dir)\n",
    "    if not radii:\n",
    "        return None\n",
    "    chosen = 500 if 500 in radii else max(radii)\n",
    "    return run_dir / f\"axa_partner_scorecard_{chosen}m.csv\"\n",
    "\n",
    "def _safe_read(path: Path) -> pd.DataFrame | None:\n",
    "    try:\n",
    "        return pd.read_csv(path) if path.exists() else None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR reading {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def _drop_bool_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove True/False columns for presentation (keep data intact in df_score).\"\"\"\n",
    "    bool_cols = [c for c in df.columns if df[c].dropna().isin([True, False]).all()]\n",
    "    # also catch dtype=bool\n",
    "    bool_cols += [c for c in df.columns if str(df[c].dtype) == \"bool\"]\n",
    "    bool_cols = sorted(set(bool_cols))\n",
    "    return df.drop(columns=bool_cols, errors=\"ignore\")\n",
    "\n",
    "# -----------------------\n",
    "# Ensure inputs exist\n",
    "# -----------------------\n",
    "if \"RUN_DIR\" in globals() and RUN_DIR is not None:\n",
    "    run_dir = Path(RUN_DIR)\n",
    "else:\n",
    "    run_dir = Path(\".\").resolve()\n",
    "\n",
    "if \"df_score\" not in globals() or df_score is None:\n",
    "    score_path = _pick_scorecard_path(run_dir)\n",
    "    df_score = _safe_read(score_path) if score_path else None\n",
    "\n",
    "if \"df_windows\" not in globals() or df_windows is None:\n",
    "    df_windows = _safe_read(run_dir / \"axa_target_windows.csv\")\n",
    "\n",
    "# -----------------------\n",
    "# Generate decision assets\n",
    "# -----------------------\n",
    "if df_score is None or df_windows is None:\n",
    "    print(\"Missing inputs:\")\n",
    "    print(\"  df_score:\", \"OK\" if df_score is not None else \"MISSING (scorecard CSV not found)\")\n",
    "    print(\"  df_windows:\", \"OK\" if df_windows is not None else \"MISSING (axa_target_windows.csv not found)\")\n",
    "    print(\"RUN_DIR used:\", run_dir)\n",
    "else:\n",
    "    score = df_score.copy()\n",
    "    win = df_windows.copy()\n",
    "\n",
    "    # Defensive typing\n",
    "    for c in [\"exposure_trips\",\"crash_count\",\"risk_rate_per_100k_trips\",\n",
    "              \"eb_risk_rate_per_100k_trips\",\"expected_incidents_proxy\",\"axa_priority_score\"]:\n",
    "        if c in score.columns:\n",
    "            score[c] = pd.to_numeric(score[c], errors=\"coerce\")\n",
    "\n",
    "    # Ensure credibility flag exists\n",
    "    if \"credibility_flag\" not in score.columns:\n",
    "        score[\"credibility_flag\"] = np.where(\n",
    "            score.get(\"exposure_trips\", 0) >= MIN_TRIPS, \"credible\", \"insufficient_data\"\n",
    "        )\n",
    "\n",
    "    # station columns\n",
    "    id_col   = \"start_station_id\" if \"start_station_id\" in score.columns else (\"station_id\" if \"station_id\" in score.columns else None)\n",
    "    name_col = \"start_station_name\" if \"start_station_name\" in score.columns else (\"station_name\" if \"station_name\" in score.columns else None)\n",
    "\n",
    "    # -----------------------------\n",
    "    # WHERE (Prevention): credible only, rank by burden (risk × exposure)\n",
    "    # -----------------------------\n",
    "    prevention = score[score[\"credibility_flag\"] == \"credible\"].copy()\n",
    "\n",
    "    if \"expected_incidents_proxy\" in prevention.columns:\n",
    "        prevention = prevention.sort_values([\"expected_incidents_proxy\",\"exposure_trips\"], ascending=False)\n",
    "        where_rank_label = \"expected burden (risk × exposure)\"\n",
    "    else:\n",
    "        prevention = prevention.sort_values([\"axa_priority_score\",\"exposure_trips\"], ascending=False)\n",
    "        where_rank_label = \"axa_priority_score\"\n",
    "\n",
    "    # Presentation columns (NO percentiles, NO booleans)\n",
    "    where_cols = [c for c in [\n",
    "        \"mode\",\"year\",\"month\",id_col,name_col,\n",
    "        \"exposure_trips\",\"crash_count\",\n",
    "        \"eb_risk_rate_per_100k_trips\",\n",
    "        \"expected_incidents_proxy\",\n",
    "        \"axa_priority_score\",\n",
    "        \"credibility_flag\"\n",
    "    ] if c and (c in prevention.columns)]\n",
    "\n",
    "    # Drop any leftover True/False columns just in case\n",
    "    prevention_show = _drop_bool_cols(prevention[where_cols].copy())\n",
    "\n",
    "    print(f\"\\nWHERE (Prevention) — Top {TOP_N} credible station-periods by {where_rank_label}\")\n",
    "    print(f\"Credibility rule: exposure_trips ≥ {MIN_TRIPS:,}\\n\")\n",
    "    display(prevention_show.head(TOP_N))\n",
    "\n",
    "    print(\"Prevention pool size:\", f\"{len(prevention):,}\", \"rows (credible)\")\n",
    "    if \"expected_incidents_proxy\" in prevention.columns:\n",
    "        print(\"Total expected burden (sum expected_incidents_proxy):\",\n",
    "              f\"{prevention['expected_incidents_proxy'].sum():,.1f}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # WHERE (Reach): top exposure (marketing / enrollment reach)\n",
    "    # -----------------------------\n",
    "    reach = score.sort_values([\"exposure_trips\"], ascending=False).copy()\n",
    "\n",
    "    reach_cols = [c for c in [\n",
    "        \"mode\",\"year\",\"month\",id_col,name_col,\n",
    "        \"exposure_trips\",\n",
    "        \"credibility_flag\"\n",
    "    ] if c and (c in reach.columns)]\n",
    "\n",
    "    reach_show = _drop_bool_cols(reach[reach_cols].copy())\n",
    "\n",
    "    print(f\"\\nWHERE (Reach) — Top {TOP_N} station-periods by exposure_trips\")\n",
    "    display(reach_show.head(TOP_N))\n",
    "\n",
    "    # -----------------------------\n",
    "    # WHEN: activation windows (rank by priority_metric if present)\n",
    "    # -----------------------------\n",
    "    for c in [\"trips\",\"pct_of_mode_year_trips\",\"pct_within_week_part\",\"priority_metric\"]:\n",
    "        if c in win.columns:\n",
    "            win[c] = pd.to_numeric(win[c], errors=\"coerce\")\n",
    "\n",
    "    w_rank = \"priority_metric\" if \"priority_metric\" in win.columns else (\"trips\" if \"trips\" in win.columns else None)\n",
    "    when = win.sort_values(w_rank, ascending=False).head(TOP_N) if w_rank else win.head(TOP_N)\n",
    "\n",
    "    # Keep WHEN table simple\n",
    "    when_cols = [c for c in [\n",
    "        \"window_type\",\"segment\",\"window_label\",\n",
    "        \"trips\",\"pct_of_mode_year_trips\",\"priority_metric\",\n",
    "        \"recommended_action\"\n",
    "    ] if c in when.columns]\n",
    "\n",
    "    when_show = _drop_bool_cols(when[when_cols].copy())\n",
    "\n",
    "    print(f\"\\nWHEN — Top {TOP_N} activation windows by {w_rank}\")\n",
    "    display(when_show)\n",
    "\n",
    "    # -----------------------------\n",
    "    # WHAT: short insurer pitch\n",
    "    # -----------------------------\n",
    "    print(\"\\nWHAT (insurer-ready plan):\")\n",
    "    print(\n",
    "        f\"- Prevention: prioritize credible stations (≥{MIN_TRIPS:,} trips) ranked by expected burden (risk × exposure).\\n\"\n",
    "        f\"- Timing: activate in the highest-traffic windows (ranked by {w_rank}) to maximize reach and conversion.\\n\"\n",
    "        f\"- Measurement: short-term conversion lift; mid-term change in incident proxy at treated stations; long-term claims frequency.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32bbbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- What m was actually used? ---\n",
    "\n",
    "# if \"eb_m_prior_used\" in df_score.columns:\n",
    "#     print(\"=== m values by scope ===\\n\")\n",
    "    \n",
    "#     m_info = df_score.groupby([\"mode\"])[\"eb_m_prior_used\"].agg([\"first\", \"min\", \"max\", \"count\"])\n",
    "#     display(m_info)\n",
    "    \n",
    "#     for mode in df_score[\"mode\"].unique():\n",
    "#         m_val = df_score[df_score[\"mode\"] == mode][\"eb_m_prior_used\"].iloc[0]\n",
    "#         n_stations = len(df_score[df_score[\"mode\"] == mode])\n",
    "        \n",
    "#         print(f\"\\nMode: {mode}\")\n",
    "#         print(f\"  Stations: {n_stations:,}\")\n",
    "#         print(f\"  m value: {m_val:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b86ae6",
   "metadata": {},
   "source": [
    "## Where are the generated plots and tables?\n",
    "\n",
    "Figures are saved under:\n",
    "\n",
    "- `reports/<RUN_TAG>/figures/`\n",
    "\n",
    "Key files:\n",
    "- `09_exposure_vs_risk_zones.png`\n",
    "- `station_zones.csv`\n",
    "- `axa_partner_scorecard_500m.csv`\n",
    "- `axa_target_windows.csv`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "modified_at": "2026-01-03T11:12:10.237164Z",
  "modified_by": "chatgpt"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
